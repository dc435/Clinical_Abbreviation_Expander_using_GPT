{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acronym Expander"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Damian Curran\n",
    "\n",
    "This is the main notebook for the paper \"Optimization and deployment challenges of closed-source LLMs for clinical note abbreviation expansion\". See readme.MD for more details.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import clear_predictions, get_accuracy, copy_corrections, get_full_set, get_main_set, get_dev_set, get_mapper, add_baseline, hard_list\n",
    "from utils.inferences import run_inferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data & set API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"Enter OpenAI API key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = get_full_set(path='./data/preprocessed_dataset_window_10.csv')\n",
    "main_set = get_main_set(full_set)\n",
    "small_set = get_dev_set(main_set, divider=10)\n",
    "mapper = get_mapper('./data/labeled_sf_lf_map_DC.csv')\n",
    "hard_set = main_set[main_set.index.isin(hard_list)]\n",
    "\n",
    "# The relevant 'data' set needs to be commented out, depending on the Experiment:\n",
    "# ------------------------------\n",
    "# data = hard_set # Experiment 1\n",
    "# data = small_set # Experiment 2\n",
    "data = main_set # Experiment 3\n",
    "# ------------------------------\n",
    "\n",
    "add_baseline(data)\n",
    "\n",
    "limit = len(data)\n",
    "print(\"Data length:\", len(data))\n",
    "print(\"Limit:     :\", limit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Inferences (Experiments 1, 2 and 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relevant 'model' set needs to be commented out, depending on the model being used in the Experiment:\n",
    "# ------------------------------\n",
    "# model = 'text-davinci-002'\n",
    "# model = 'text-davinci-003'\n",
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "# Prompt amendments are made manually in inferences.py prior to new round of inferences.\n",
    "\n",
    "clear_predictions(data)\n",
    "error_indices = [] # to keep track of any instances where parsing error occurs in post-processing.\n",
    "data = run_inferences(data, mapper, api_key, error_indices, model = model, batch_size=5, limit=limit, verbose=True, save_freq=2000)\n",
    "if len(error_indices) > 0:\n",
    "    data_errors = run_inferences(data.loc[error_indices], mapper, api_key, error_indices, model = model, batch_size=1, limit=limit, verbose=False, save_freq=1000)\n",
    "    copy_corrections(data_errors,data)\n",
    "get_accuracy(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate results from dataframe and save as .csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import build_results\n",
    "results = build_results(data,mapper)\n",
    "results.head(43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Inferences on rare_set (Experiment 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import get_full_set, get_rare_mapper\n",
    "rare_set = get_full_set(path='./data/rare_data.csv',long=False)\n",
    "rare_mapper = get_rare_mapper\n",
    "limit = len(rare_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo'\n",
    "\n",
    "clear_predictions(rare_set)\n",
    "error_indices = []\n",
    "data = run_inferences(rare_set, rare_mapper, api_key, error_indices, model = model, batch_size=5, limit=limit, verbose=True, save_freq=2000)\n",
    "if len(error_indices) > 0:\n",
    "    data_errors = run_inferences(rare_set.loc[error_indices], rare_mapper, api_key, error_indices, model = model, batch_size=1, limit=limit, verbose=True, save_freq=1000)\n",
    "    copy_corrections(data_errors,rare_set)\n",
    "get_accuracy(rare_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_109_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
